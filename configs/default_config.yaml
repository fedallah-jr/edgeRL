# Default configuration for EdgeSim-RL

# Environment configuration
env:
  name: "EdgeEnv"
  dataset_path: "datasets/sample_dataset.json"
  max_steps: 1000
  tick_duration: 1
  tick_unit: "seconds"
  
  # Enable environment stochasticity (ON by default)
  randomize_initial_placement: true   # Randomize service initial placement at reset
  use_random_scheduler: true          # Use EdgeSimPy's RandomScheduler for agent activation order
  
  # State space configuration
  state:
    include_server_metrics: true
    include_service_metrics: true
    normalize: true
    include_user_server_distance_matrix: true
  
  # Action space configuration  
  action:
    type: "discrete"  # discrete or continuous

# Reward configuration
reward:
  # Available reward types: power, latency, inverse_power
  type: "inverse_power"  # power, latency, or inverse_power
  mode: "sum_of_inverses"  # inverse_of_sum or sum_of_inverses
  power_weight: 1.0      # Multiplier for power reward
  power_scale: 1000.0    # Scale for normalization of power (similar to latency_scale)
  # Inverse-power reward optional params (used when type: inverse_power)
  inverse_power_weight: 1.0  # Multiplier for inverse-power reward (falls back to power_weight)
  inv_power_scale: 0.1       # Scale for normalization of inverse-power reward (falls back to power_scale)
  normalize: true      # Used by all rewards for scaling
  penalty_invalid_action: 0.0  # Applied when actions are invalid (added to the reward)

# Training configuration
training:
  seed: 42
  num_episodes: 400
  checkpoint_freq: 100
  checkpoint_dir: "checkpoints/"
  log_dir: "logs/"
  
# Resource allocation
resources:
  num_workers: 2
  num_gpus: 0
  num_cpus_per_worker: 1
  
# Evaluation configuration
evaluation:
  eval_episodes: 5
  eval_interval: 50
  save_best_model: true